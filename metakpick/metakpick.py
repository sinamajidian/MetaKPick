

import argparse
import sys
import os
import logging
import pandas as pd
import numpy as np
import random
import pickle
from datetime import datetime
np.random.seed(42)

import _training
import _classifier
import _utils_tree
import _utils_kraken

# import matplotlib.pyplot as plt
# import seaborn as sns

# import pickle
# from collections import Counter
# import random

np.random.seed(42)



def main():

    logging.basicConfig(level=logging.DEBUG,format='%(asctime)s - %(levelname)s - %(message)s')

    parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
    # # Main operation mode arguments
    #mode_group = parser.add_mutually_exclusive_group(required=True)
    #mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    #mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')
    parser.add_argument('--mode', type=str, help='train or classify')
    # parser.add_argument('--output', type=str, help='Output file')
    # parser.add_argument('--model', type=str, help='Model file')
    # parser.add_argument('--threads', type=int, help='Number of threads')

    args = parser.parse_args()
    mode=args.mode # train or classify
    
    
    logging.info("Starting the program and loading the tax info")
    workingdir="/vast/blangme2/smajidi5/metagenomics/metakpick_project/"
    in_folder= workingdir+"files/"
    tree_file=in_folder+"nodes.dmp"
    tax_genome_file= in_folder + "seqid2taxid.map_tax_uniq"

    info, Tree, tax_index, tax_genome, parents, tree_df = _utils_tree.get_tax_info(tree_file,tax_genome_file)    
    tax2path, tax2depth = _utils_tree.get_tax2path(tax_genome, info, parents)

    if mode=="train": 
        folder_input =workingdir+"../changek/simulatation/classification/max15/"
        truth_file=folder_input+"true_tax_max15.csv" # generated by generate_training.ipynb
        logging.info("Reading the truth file")
        dic_tax_truth = _utils_kraken.read_truth_file(truth_file)
        logging.info("Number of reads in the truth file: "+str(len(dic_tax_truth)))

        classification_folder=folder_input 
        cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]

        cases=cases[2:]#[4:5]+cases[-1:]
        #logging.info("Reading the kraken classification")
        #merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
        # # todo: get the read set first, then 
        # logging.info("Limiting the reads per genome")
        # num_max=5
        # merged_limited, readids_max = _utils_kraken.limit_read_per_genome(merged,num_max)
            
        # read_names_list=list(readids_max)
        # #cases_dic = _utils_kraken.calculate_tp_fp(dfmerged_taxa,info,tree_df,parents,tax_level,col_name,tax_index)
        # logging.info("Finding the true k size")
        # tp_cases_dic, true_k  =  _utils_kraken.find_true_ksize(cases,merged_limited,info,tree_df,parents,tax_index, readids_max, level='species')
        # logging.info("Reading the kraken kmers")
        # kraken_kmers_cases, read_names_cases = _utils_kraken.read_kraken_all(cases, classification_folder, readids_max)
        # logging.info("Getting the features")
        #logging.info("Mode train started")
        logging.info("Reading the kraken kmers")
        read_names_list, kraken_kmers_cases = _utils_kraken.read_kraken_all(cases, classification_folder)
        # logging.info("Getting the tax depth")
        read_tax_depth = _utils_kraken.get_tax_depth(kraken_kmers_cases, info,parents)
        tax_level='species'
        reads_tp_cases = _utils_kraken.calculate_true_k(kraken_kmers_cases,dic_tax_truth,info,tree_df,parents,tax_level,tax_index,read_names_list)
        
        logging.info("Getting the tp binary reads cases")
        tp_binary_reads_cases = _training.get_tp_binary_reads_cases(cases, read_names_list, reads_tp_cases)
        features_cases, feature_names = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, read_tax_depth, tax2depth, info, parents)
        logging.info("Cases in features: "+str(features_cases.keys()))
        
        logging.info("Training the RF model")
        regr_dic = _training.train_RF_model_all(features_cases, tp_binary_reads_cases,read_names_list,n_estimators=1000, max_features=float(0.8), max_leaf_nodes=50, random_state=14, n_jobs=20) 
        logging.info("Saving the model")
        logging.info(regr_dic)

        now = datetime.now()
        time_stamp_model = now.strftime("%Y%m%d_%H%M%S")
        logging.info("Saving the model in: "+workingdir+"results/Random_Forest_regression_models"+time_stamp_model+".pkl")
        pickle.dump(regr_dic, open(workingdir+"results/Random_Forest_regression_models"+time_stamp_model+".pkl", "wb"))

        
    
    elif mode=="classify":

        folder_input =workingdir+"../changek/simulatation/classification/max15/"
        truth_file=folder_input+"true_tax_max15.csv" # generated by generate_training.ipynb
        logging.info("Reading the truth file")
        dic_tax_truth = _utils_kraken.read_truth_file(truth_file)
        logging.info("Number of reads in the truth file: "+str(len(dic_tax_truth)))

        classification_folder=folder_input 
        cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]

        #cases=cases[4:5]+cases[-1:]
        #logging.info("Classifying the reads")
        #logging.info("Loading the model")
        model_folder=workingdir+"results/"
        model_files =[ model_file for  model_file in os.listdir(model_folder) if model_file.endswith(".pkl") and model_file.startswith("Random_Forest_regression_models")]
        model_files.sort()
        model_file=model_files[-1]
        #model_time_stamp=model_file.split("_")[1] # model_file=workingdir+"Random_Forest_regression_models"+model_time_stamp+".pkl" 
        logging.info("Loading the model: "+model_file+" in the folder: "+model_folder)
        loaded_regression_dic= pickle.load(open(model_folder+model_file, "rb"))
        logging.info("Model loaded")
        
        classify_folder=workingdir+"../changek/simulatation/classification/max15/"
        cases_classify =[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')]
        cases_model= list(loaded_regression_dic.keys())
        cases_classify_intersect=set(cases_classify).intersection(set(cases_model))
        logging.info("Cases in the input for classification: "+str(cases_classify))
        logging.info("Cases in the model: "+str(cases_model))
        logging.warning("Working on the intersection of cases in the input for classification and in the model: "+str(cases_classify_intersect))

        
        logging.info("Reading the kraken kmers for these cases")
        read_names_list, kraken_kmers_cases = _utils_kraken.read_kraken_all(cases_classify_intersect, classify_folder)
        logging.info("Number of reads in the kraken kmers: "+str(len(kraken_kmers_cases)))
        logging.info("Getting the tax depth")
        read_tax_depth = _utils_kraken.get_tax_depth(kraken_kmers_cases, info,parents)
        logging.info("Getting the features")
        features_cases, feature_names = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, read_tax_depth, tax2depth, info, parents)
        logging.info("Cases in features: "+str(features_cases.keys()))
        logging.info("Applying the model")
        
        read_k_prob= _classifier.apply_RF_model(features_cases,read_names_list,loaded_regression_dic)
        logging.info("Number of reads in the read_k_prob: "+str(len(read_k_prob)))

        logging.info("Getting the best tax")
        best_k_dic, estimated_tax_dict = _classifier.get_best_tax(read_k_prob,read_names_list,kraken_kmers_cases,thr_minprob=0.5) 
        logging.info("Writing the estimated tax")
        output_file_name=workingdir+"results/estimated_tax.csv"
        output_file_name= _training.write_estimated_tax(estimated_tax_dict,output_file_name)


        logging.info("Number of reads in the dic_tax_estimated: "+str(len(estimated_tax_dict)))
        read_tpfp_dic= _utils_kraken.calculate_tp_fp('predicted_tax',estimated_tax_dict,dic_tax_truth,info,tree_df,parents,'species',tax_index)
        logging.info("Number of reads in the TP: "+str(len(read_tpfp_dic['TP'])))
        
        logging.info("Calculating the tp fp")
        tool="RF"
        # # for case1 in ['taxon_tool_k'+str(k) for k in  [19,21,25,31]]+["oracle_____","RF_allk", "Random_allk" , "RF_allk",'RF_allk_tog']:
        # for case1 in ['RF' ]: # ['taxon_tool_'+case for case in cases]  #,'RF_oneByone' , "RF_allk",'RF_allk_tog'
        FP=len(read_tpfp_dic['FP-level-index'])+len(read_tpfp_dic['FP-higher-index'])+len(read_tpfp_dic['FP-level-notindex'])+len(read_tpfp_dic['FP-higher-notindex'])
        recall=len(read_tpfp_dic['TP'])/(len(read_tpfp_dic['TP']) + len(read_tpfp_dic['VP']) + len(read_tpfp_dic['FN']) +FP )
        if len(read_tpfp_dic['TP'])!=0:
            precision=len(read_tpfp_dic['TP'])/(len(read_tpfp_dic['TP']) + FP)
            F1= 2* precision* recall/(precision+recall)
        else:
            precision=0
            F1=0
        print(tool,'\t',round(F1,4), round(precision,4),round(recall,4),len(read_tpfp_dic['TP']),FP,len(read_tpfp_dic['VP']))
                
        # _training.plot_tree(regr_dic, feature_names, workingdir,num_trees=1)

if __name__ == "__main__":
    main()
