

import argparse
import sys
import os
import logging
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import pickle
from collections import Counter
import random

from _utils_tree import parse_tree_file, find_tax2root


np.random.seed(42)



def main():
    # parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
 
    # # Main operation mode arguments
    # mode_group = parser.add_mutually_exclusive_group(required=True)
    # mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    # mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')

    # parser.add_argument('--reads', type=str, help='Input reads file')
    # parser.add_argument('--output', type=str, help='Output file')
    #parser.add_argument('--model', type=str, help='Model file')
   # parser.add_argument('--threads', type=int, help='Number of threads')

    tree_file="/vast/blangme2/smajidi5/metagenomics/changek/kraken1/kraken_db/standard/k31/taxonomy/nodes.dmp"
    tax_genome_file="/vast/blangme2/smajidi5/metagenomics/changek/kraken1/kraken_db/standard/seqid2taxid.map_tax_uniq"
    # tax_genome_file="/vast/blangme2/smajidi5/metagenomics/changek/kraken1/kraken_db/48genomes/added_header_tax_uniq"
    # tree_file= "/vast/blangme2/smajidi5/metagenomics/changek/kraken1/kraken_db/48genomes/k31/taxonomy/nodes.dmp"

    tax_index = get_tax_index(tree_file, tax_genome_file)
    print("Tax index: ", tax_index)

mode="train"
if mode=="train":
    tax2root_all, tax2root_all_dic = get_tax2root_all(info, parents, tax_genome)
    tax2path, tax2depth = get_tax2path_all(tax2root_all)

    folder_input="/vast/blangme2/smajidi5/metagenomics/changek/simulatation/"
    truth_file=folder_input+"true_tax.csv" # generated by generate_training.ipynb
    folder=folder_input+"classification/" # 

    num_max=15
    merged = merge_kraken_results(folder, truth_file)
    readids_max = get_readids_max(merged, num_max)
    merged2=merged.copy()
    merged2a= merged2[merged2['read_name'].isin(readids_max)]
    merged2a.reset_index(drop=True, inplace=True)
    len(merged2a),len(merged)#fastq_file= folder_input+"reads.fq"

    read_tax_depth = get_read_tax_depth(merged2a, info, parents, cases)
    print(read_tax_depth)

    #fastq_file= folder_input+"reads.fq"
    dic_matrix2={}
for case in cases[::-1]: 
    print(case)
    kmer_size= int(case[1:])
    X3=[]
    for read_idx,read_id in enumerate(read_names_):
        if read_idx%10000==0:
            print(read_idx,len(read_names_))
        if read_id not in dic_cases[case]:
            features=np.zeros(len(feature_names))
            X3.append(features)
            continue 
    

        features=get_features(read_id, dic_cases[case], kmer_size, tax2path, tax2depth, read_tax_depth[case], tax_index)        
        features_tax = get_features_kmer(tax_krak, rlen, tax_kmer_dic, tax_kmer_num_dic, num_nodes_tree, tax2depth)
        features_tax2 = get_features_depth(tax_krak, rlen, tax_kmer_dic, tax_kmer_num_dic, num_nodes_tree, tax2depth)
        #len(features),len(feature_names),features
        features = np.concatenate([features, features_tax, features_tax2])
    
        X3.append(features)
    X3=np.array(X3)
    dic_matrix2[case]=X3

print(len(dic_matrix2),len(dic_matrix2[case]),len(dic_matrix2[case][0]))



if __name__ == "__main__":
    main()
