

import argparse
import sys
import os
import logging
import pandas as pd
import numpy as np
import random
import pickle
from datetime import datetime
np.random.seed(42)

import _training
import _utils
import _utils_tree
import _utils_kraken

# import matplotlib.pyplot as plt
# import seaborn as sns

# import pickle
# from collections import Counter
# import random

np.random.seed(42)



def main():

    logging.basicConfig(level=logging.DEBUG,format='%(asctime)s - %(levelname)s - %(message)s')

    # parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
    # # Main operation mode arguments
    # mode_group = parser.add_mutually_exclusive_group(required=True)
    # mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    # mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')
    # parser.add_argument('--reads', type=str, help='Input reads file')
    # parser.add_argument('--output', type=str, help='Output file')
    # parser.add_argument('--model', type=str, help='Model file')
    # parser.add_argument('--threads', type=int, help='Number of threads')

    
    workingdir="/vast/blangme2/smajidi5/metagenomics/metakpick_project/MetaKPick/"
    in_folder= workingdir+"../files/"
    tree_file=in_folder+"nodes.dmp"
    tax_genome_file= in_folder + "seqid2taxid.map_tax_uniq"

    info, Tree, tax_index, tax_genome, parents, tree_df = _utils_tree.get_tax_info(tree_file,tax_genome_file)    
    tax2path, tax2depth = _utils_tree.get_tax2path(tax_genome, info, parents)



    mode="train"
    if mode=="train":
        logging.info("Training the model")
        folder_input =workingdir+"../../changek/simulatation/"
        truth_file=folder_input+"true_tax.csv" # generated by generate_training.ipynb
        classification_folder=folder_input+"classification/" 
        cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]
        cases=cases
        logging.info("Reading the kraken classification")
        merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
        logging.info("Getting the tax depth")
        read_tax_depth = _utils_kraken.get_tax_depth(merged,cases, info,parents)
        # todo: get the read set first, then 
        logging.info("Limiting the reads per genome")
        num_max=3
        merged_limited, readids_max = _utils_kraken.limit_read_per_genome(merged,num_max)
        read_names_list=list(readids_max)
        #cases_dic = _utils_kraken.calculate_tp_fp(dfmerged_taxa,info,tree_df,parents,tax_level,col_name,tax_index)
        logging.info("Finding the true k size")
        tp_cases_dic, true_k  =  _utils_kraken.find_true_ksize(cases,merged_limited,info,tree_df,parents,tax_index, readids_max, level='species')
        logging.info("Reading the kraken kmers")
        kraken_kmers_cases, read_names_cases = _utils_kraken.read_kraken_all(cases, classification_folder, readids_max)
        logging.info("Getting the features")
        features = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, cases, read_tax_depth, tax2depth, info, parents, classification_folder)
        #features = get_features_all(read_names_list, tax2path, kraken_kmers_cases, cases, read_tax_depth, tax2depth, info, parents, classification_folder)
        print(len(features))
        logging.info("Getting the tp binary reads cases")
        tp_binary_reads_cases = _training.get_tp_binary_reads_cases(cases, read_names_list, tp_cases_dic)
        logging.info("Training the RF model")
        regr_dic,read_k_prob = _training.train_RF_model_all(cases, features, tp_binary_reads_cases,read_names_list,n_estimators=1000, max_features=float(0.8), max_leaf_nodes=50, random_state=14, n_jobs=20) 
        logging.info("Saving the model")
        print(regr_dic)

        now = datetime.now()
        time_stamp_model = now.strftime("%Y%m%d_%H%M%S")
        logging.info("Saving the model in: "+workingdir+"Random_Forest_regression_models"+time_stamp_model+".pkl")
        pickle.dump(regr_dic, open(workingdir+"Random_Forest_regression_models"+time_stamp_model+".pkl", "wb"))
        logging.info("Getting the best tax")
        best_k_dic,estimated_tax_dict =_training.get_best_tax(read_k_prob,cases,read_names_list,merged,thr_minprob=0.5)
        logging.info("Writing the estimated tax")
        output_file_name= _utils.write_estimated_tax(estimated_tax_dict,output_file_name=workingdir+"estimated_tax.csv")
        logging.info("Done")

    mode='test'
    if mode=="test":

        logging.info("Testing the model")
        truth_file=folder_input+"true_tax.csv" # generated by generate_training.ipynb
        #merged = _utils_kraken.read_truth_file(truth_file) 
        merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
        output_file_name=workingdir+"estimated_tax.csv"
        estimated_tax_dict = _utils.read_estimated_tax(output_file_name)
        print(len(estimated_tax_dict))
        estimated=[]
        for read_name, taxon in merged.iterrows():
            if read_name not in estimated_tax_dict:
                estimated.append(estimated_tax_dict[taxon])
            else:   
                estimated.append(0)
        merged['RF']=estimated
        
        # for case1 in ['taxon_tool_k'+str(k) for k in  [19,21,25,31]]+["oracle_____","RF_allk", "Random_allk" , "RF_allk",'RF_allk_tog']:
        for case1 in ['taxon_tool_'+case for case in cases]+['RF' ]:  #,'RF_oneByone' , "RF_allk",'RF_allk_tog'
            cases_dic_=_utils_kraken.calculate_tp_fp(merged,info,tree_df,parents,'species',case1,tax_index) #+case
            FP=len(cases_dic_['FP-level-index'])+len(cases_dic_['FP-higher-index'])+len(cases_dic_['FP-level-notindex'])+len(cases_dic_['FP-higher-notindex'])
            recall=len(cases_dic_['TP'])/(len(cases_dic_['TP']) + len(cases_dic_['VP']) + len(cases_dic_['FN']) +FP )
            precision=len(cases_dic_['TP'])/(len(cases_dic_['TP']) + FP)
            F1= 2* precision* recall/(precision+recall)
            if 'taxon' in case1:
                print(case1[-3:],'\t\t',round(F1,4), round(precision,4),round(recall,4),len(cases_dic_['TP']),FP,len(cases_dic_['VP']))
            else:
                print(case1,'\t',round(F1,4), round(precision,4),round(recall,4),len(cases_dic_['TP']),FP,len(cases_dic_['VP']))
                
        
    elif mode=="classify":
        logging.info("Classifying the reads")
        logging.info("Loading the model")
        model_files =[ model_file for  model_file in os.listdir(workingdir) if model_file.endswith(".pkl") and model_file.startswith("Random_Forest_regression_models")]
        model_files.sort()
        model_file=model_files[-1]
        model_time_stamp=model_file.split("_")[1]
        model_file=workingdir+"Random_Forest_regression_models"+model_time_stamp+".pkl" 
        print(model_file)
        regr_dic= pickle.load(open(model_file, "rb"))
        #output_file_name= _utils.write_estimated_tax(estimated_tax_dict,output_file_name=workingdir+"estimated_tax.csv")



if __name__ == "__main__":
    main()
