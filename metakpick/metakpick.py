

import argparse
import sys
import os
import logging
import pandas as pd
import numpy as np
import random
np.random.seed(42)
import _training
#from _utils_tree import parse_tree_file, find_tax2root
import _utils_tree
import _utils_kraken

# import matplotlib.pyplot as plt
# import seaborn as sns

# import pickle
# from collections import Counter
# import random

np.random.seed(42)



def main():


    
    workingdir="/vast/blangme2/smajidi5/metagenomics/metakpick_project/MetaKPick/"
    in_folder= workingdir+"../files/"
    tree_file=in_folder+"nodes.dmp"
    tax_genome_file= in_folder + "seqid2taxid.map_tax_uniq"

    info, Tree, tax_index, tax_genome, parents, tree_df = _utils_tree.get_tax_info(tree_file,tax_genome_file)    
    tax2path, tax2depth = _utils_tree.get_tax2path(tax_genome, info, parents)

    folder_input=workingdir+"../../changek/simulatation/"
    truth_file=folder_input+"true_tax.csv" # generated by generate_training.ipynb
    classification_folder=folder_input+"classification/" 
    cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]
    cases=cases[-2:]
    merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
    read_tax_depth = _utils_kraken.get_tax_depth(merged,cases, info,parents)


    num_max=2
    merged2a, readids_max = _utils_kraken.limit_read_per_genome(merged,num_max)

    #cases_dic = _utils_kraken.def_cal(dfmerged_taxa,info,tree_df,parents,tax_level,col_name,tax_index)
    case_dic_all, true_k  =  _utils_kraken.find_true_ksize(cases,merged2a,info,tree_df,parents,tax_index, readids_max, level='species')

    dic_cases, cases_readids = _training.read_kraken_all(cases, classification_folder, readids_max)
    num_nodes_tree = len(tax2path) # 52229 

    dic_matrix2={}
    for case in cases[::-1]: 
        print(case)
        kmer_size= int(case[1:])
        X3=[]
        for read_idx,read_id in enumerate(read_names_):
            if read_idx%10000==0:
                print(read_idx,len(read_names_))
            if read_id not in dic_cases[case]:
                features=np.zeros(len(feature_names))
                X3.append(features)
                continue 

            tax_krak, rlen, tax_kmer_dic, tax_kmer_num_dic = dic_cases[case][read_id] # read length
            features=_training.get_features(dic_cases, case, read_id, num_nodes_tree)
            features_depth=_training.get_features_depth(dic_cases, case, read_id, read_tax_depth, tax2depth)
            features_tax=_training.get_features_tax(dic_cases, case, read_id, read_tax_depth, tax2depth, info, parents,  tax_kmer_num_dic)
            features=np.concatenate([features, features_depth, features_tax])

            X3.append(features)
        X3=np.array(X3)
        dic_matrix2[case]=X3    

    print(len(dic_matrix2),len(dic_matrix2[case]),len(dic_matrix2[case][0]))

    # parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
    # # Main operation mode arguments
    # mode_group = parser.add_mutually_exclusive_group(required=True)
    # mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    # mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')
    # parser.add_argument('--reads', type=str, help='Input reads file')
    # parser.add_argument('--output', type=str, help='Output file')
    # parser.add_argument('--model', type=str, help='Model file')
    # parser.add_argument('--threads', type=int, help='Number of threads')



    # mode="train"
    # if mode=="train":
    #     tax2root_all, tax2root_all_dic = get_tax2root_all(info, parents, tax_genome)
    #     tax2path, tax2depth = get_tax2path_all(tax2root_all)

    #     folder_input="/vast/blangme2/smajidi5/metagenomics/changek/simulatation/"
    #     truth_file=folder_input+"true_tax.csv" # generated by generate_training.ipynb
    #     folder=folder_input+"classification/" # 

    #     num_max=15
    #     merged = merge_kraken_results(folder, truth_file)
    #     readids_max = get_readids_max(merged, num_max)
    #     merged2=merged.copy()
    #     merged2a= merged2[merged2['read_name'].isin(readids_max)]
    #     merged2a.reset_index(drop=True, inplace=True)
    #     len(merged2a),len(merged)#fastq_file= folder_input+"reads.fq"

    #     read_tax_depth = get_read_tax_depth(merged2a, info, parents, cases)
    #     print(read_tax_depth)

    #     #fastq_file= folder_input+"reads.fq"
    #     dic_matrix2={}
    # for case in cases[::-1]: 
    #     print(case)
    #     kmer_size= int(case[1:])
    #     X3=[]
    #     for read_idx,read_id in enumerate(read_names_):
    #         if read_idx%10000==0:
    #             print(read_idx,len(read_names_))
    #         if read_id not in dic_cases[case]:
    #             features=np.zeros(len(feature_names))
    #             X3.append(features)
    #             continue 
        

    #         features=get_features(read_id, dic_cases[case], kmer_size, tax2path, tax2depth, read_tax_depth[case], tax_index)        
    #         features_tax = get_features_kmer(tax_krak, rlen, tax_kmer_dic, tax_kmer_num_dic, num_nodes_tree, tax2depth)
    #         features_tax2 = get_features_depth(tax_krak, rlen, tax_kmer_dic, tax_kmer_num_dic, num_nodes_tree, tax2depth)
    #         #len(features),len(feature_names),features
    #         features = np.concatenate([features, features_tax, features_tax2])
        
    #         X3.append(features)
    #     X3=np.array(X3)
    #     dic_matrix2[case]=X3

    # print(len(dic_matrix2),len(dic_matrix2[case]),len(dic_matrix2[case][0]))



if __name__ == "__main__":
    main()
