

import argparse
import sys
import os
import logging
import pandas as pd
import numpy as np
import random
import pickle
from datetime import datetime
np.random.seed(42)

import _training
import _classifier
import _utils_tree
import _utils_kraken

# import matplotlib.pyplot as plt
# import seaborn as sns

# import pickle
# from collections import Counter
# import random

np.random.seed(42)



def main():

    logging.basicConfig(level=logging.DEBUG,format='%(asctime)s - %(levelname)s - %(message)s')

    # parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
    # # Main operation mode arguments
    # mode_group = parser.add_mutually_exclusive_group(required=True)
    # mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    # mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')
    # parser.add_argument('--reads', type=str, help='Input reads file')
    # parser.add_argument('--output', type=str, help='Output file')
    # parser.add_argument('--model', type=str, help='Model file')
    # parser.add_argument('--threads', type=int, help='Number of threads')

    mode='train'
    #mode="test"
    if mode=='train' or mode=='test':
        workingdir="/vast/blangme2/smajidi5/metagenomics/metakpick_project/"
        in_folder= workingdir+"files/"
        tree_file=in_folder+"nodes.dmp"
        tax_genome_file= in_folder + "seqid2taxid.map_tax_uniq"

        info, Tree, tax_index, tax_genome, parents, tree_df = _utils_tree.get_tax_info(tree_file,tax_genome_file)    
        tax2path, tax2depth = _utils_tree.get_tax2path(tax_genome, info, parents)

        folder_input =workingdir+"../changek/simulatation/classification/max15/"
        truth_file=folder_input+"true_tax_max15.csv" # generated by generate_training.ipynb

        dic_tax_truth = _utils_kraken.read_truth_file(truth_file)
        logging.info("Number of reads in the truth file: "+str(len(dic_tax_truth)))

        classification_folder=folder_input 
        cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]

        cases=cases[4:5]+cases[-1:]
        #logging.info("Reading the kraken classification")
        #merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
        # # todo: get the read set first, then 
        # logging.info("Limiting the reads per genome")
        # num_max=5
        # merged_limited, readids_max = _utils_kraken.limit_read_per_genome(merged,num_max)
            
        # read_names_list=list(readids_max)
        # #cases_dic = _utils_kraken.calculate_tp_fp(dfmerged_taxa,info,tree_df,parents,tax_level,col_name,tax_index)
        # logging.info("Finding the true k size")
        # tp_cases_dic, true_k  =  _utils_kraken.find_true_ksize(cases,merged_limited,info,tree_df,parents,tax_index, readids_max, level='species')
        # logging.info("Reading the kraken kmers")
        # kraken_kmers_cases, read_names_cases = _utils_kraken.read_kraken_all(cases, classification_folder, readids_max)
        # logging.info("Getting the features")

    if mode=="train":
        logging.info("Training the model")


        read_names_list, kraken_kmers_cases = _utils_kraken.read_kraken_all(cases, classification_folder)
        # logging.info("Getting the tax depth")
        read_tax_depth = _utils_kraken.get_tax_depth(kraken_kmers_cases, info,parents)
        tax_level='species'
        reads_tp_cases = _utils_kraken.calculate_true_k(kraken_kmers_cases,dic_tax_truth,info,tree_df,parents,tax_level,tax_index,read_names_list)
        
        features_cases, feature_names = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, read_tax_depth, tax2depth, info, parents)
        
        logging.info("Cases in features: "+str(features_cases.keys()))
        logging.info("Getting the tp binary reads cases")
        tp_binary_reads_cases = _training.get_tp_binary_reads_cases(read_names_list, reads_tp_cases)
        logging.info("Training the RF model")
        regr_dic,read_k_prob = _training.train_RF_model_all(features_cases, tp_binary_reads_cases,read_names_list,n_estimators=1000, max_features=float(0.8), max_leaf_nodes=50, random_state=14, n_jobs=20) 
        logging.info("Saving the model")
        logging.info(regr_dic)

        now = datetime.now()
        time_stamp_model = now.strftime("%Y%m%d_%H%M%S")
        logging.info("Saving the model in: "+workingdir+"../results/Random_Forest_regression_models"+time_stamp_model+".pkl")
        pickle.dump(regr_dic, open(workingdir+"../results/Random_Forest_regression_models"+time_stamp_model+".pkl", "wb"))
        logging.info("Getting the best tax")
        best_k_dic,estimated_tax_dict = _classifier.get_best_tax(read_k_prob,cases,read_names_list,kraken_kmers_cases,thr_minprob=0.5) 
        logging.info("Writing the estimated tax")
        output_file_name= _training.write_estimated_tax(estimated_tax_dict,output_file_name=workingdir+"../results/estimated_tax.csv")
        logging.info("Done")

    
    if mode=="test":

        logging.info("Testing the model")
        logging.info("Reading the estimated tax")
        output_file_name=workingdir+"../results/estimated_tax.csv"
        estimated_tax_dict = _training.read_estimated_tax(output_file_name=workingdir+"../results/estimated_tax.csv")
        print(len(estimated_tax_dict))
        estimated=[]
        # for row_idx, row in merged_limited.iterrows():
        #     read_name=row['read_name']

        #     if read_name in estimated_tax_dict:
        #         estimated.append(estimated_tax_dict[read_name])
        #     else:   
        #         estimated.append(0)
        # merged_limited['RF']=estimated

        # print(merged_limited.head())

        # logging.info("Calculating the tp fp")
        # # for case1 in ['taxon_tool_k'+str(k) for k in  [19,21,25,31]]+["oracle_____","RF_allk", "Random_allk" , "RF_allk",'RF_allk_tog']:
        # for case1 in ['RF' ]: # ['taxon_tool_'+case for case in cases]  #,'RF_oneByone' , "RF_allk",'RF_allk_tog'
        #     cases_dic_=_utils_kraken.calculate_tp_fp(merged_limited,info,tree_df,parents,'species',case1,tax_index) #+case
        #     FP=len(cases_dic_['FP-level-index'])+len(cases_dic_['FP-higher-index'])+len(cases_dic_['FP-level-notindex'])+len(cases_dic_['FP-higher-notindex'])
        #     recall=len(cases_dic_['TP'])/(len(cases_dic_['TP']) + len(cases_dic_['VP']) + len(cases_dic_['FN']) +FP )
        #     if len(cases_dic_['TP'])!=0:
        #         precision=len(cases_dic_['TP'])/(len(cases_dic_['TP']) + FP)
        #     else:
        #         precision=0
        #     if precision+recall!=0:
        #         F1= 2* precision* recall/(precision+recall)
        #     else:
        #         F1=0
        #     if 'taxon' in case1:
        #         print(case1[-3:],'\t\t',round(F1,4), round(precision,4),round(recall,4),len(cases_dic_['TP']),FP,len(cases_dic_['VP']))
        #     else:
        #         print(case1,'\t',round(F1,4), round(precision,4),round(recall,4),len(cases_dic_['TP']),FP,len(cases_dic_['VP']))
                
        # _training.plot_tree(regr_dic, feature_names, workingdir,num_trees=1)

    elif mode=="classify":
        logging.info("Classifying the reads")
        logging.info("Loading the model")
        model_files =[ model_file for  model_file in os.listdir(workingdir) if model_file.endswith(".pkl") and model_file.startswith("Random_Forest_regression_models")]
        model_files.sort()
        model_file=model_files[-1]
        model_time_stamp=model_file.split("_")[1]
        model_file=workingdir+"Random_Forest_regression_models"+model_time_stamp+".pkl" 
        print(model_file)
        regr_dic= pickle.load(open(model_file, "rb"))
        #output_file_name= _training.write_estimated_tax(estimated_tax_dict,output_file_name=workingdir+"estimated_tax.csv")
        classify_folder=workingdir+"../../changek/simulatation/classification/"

        cases_classify =[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')]

        cases_model= list(regr_dic.keys())
        cases_classify_intersect=set(cases_classify).intersection(set(cases_model))
        logging.info("Cases in the input for classification: "+str(cases_classify))
        logging.info("Cases in the model: "+str(cases_model))
        logging.info("Cases in the input for classification and in the model: "+str(cases_classify_intersect))

        
        logging.info("Reading the kraken kmers")
        kraken_kmers_cases, read_names_cases = _utils_kraken.read_kraken_all(cases_classify_intersect, classify_folder, set(), 1e10 )
        print(kraken_kmers_cases)



if __name__ == "__main__":
    main()
