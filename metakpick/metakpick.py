

import argparse
import sys
import os
import logging
import pandas as pd
import numpy as np
import random
np.random.seed(42)

import _training
import _utils
import _utils_tree
import _utils_kraken

# import matplotlib.pyplot as plt
# import seaborn as sns

# import pickle
# from collections import Counter
# import random

np.random.seed(42)



def main():




    # parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
    # # Main operation mode arguments
    # mode_group = parser.add_mutually_exclusive_group(required=True)
    # mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    # mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')
    # parser.add_argument('--reads', type=str, help='Input reads file')
    # parser.add_argument('--output', type=str, help='Output file')
    # parser.add_argument('--model', type=str, help='Model file')
    # parser.add_argument('--threads', type=int, help='Number of threads')

    
    workingdir="/vast/blangme2/smajidi5/metagenomics/metakpick_project/MetaKPick/"
    in_folder= workingdir+"../files/"
    tree_file=in_folder+"nodes.dmp"
    tax_genome_file= in_folder + "seqid2taxid.map_tax_uniq"

    info, Tree, tax_index, tax_genome, parents, tree_df = _utils_tree.get_tax_info(tree_file,tax_genome_file)    
    tax2path, tax2depth = _utils_tree.get_tax2path(tax_genome, info, parents)



    mode="train"
    if mode=="train":

        folder_input =workingdir+"../../changek/simulatation/"
        truth_file=folder_input+"true_tax.csv" # generated by generate_training.ipynb
        classification_folder=folder_input+"classification/" 
        cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]
        cases=cases#[-2:]
        merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
        read_tax_depth = _utils_kraken.get_tax_depth(merged,cases, info,parents)
        # todo: get the read set first, then 
        num_max=2
        merged_limited, readids_max = _utils_kraken.limit_read_per_genome(merged,num_max)
        read_names_list=list(readids_max)
        #cases_dic = _utils_kraken.def_cal(dfmerged_taxa,info,tree_df,parents,tax_level,col_name,tax_index)
        tp_cases_dic, true_k  =  _utils_kraken.find_true_ksize(cases,merged_limited,info,tree_df,parents,tax_index, readids_max, level='species')
        kraken_kmers_cases, read_names_cases = _utils_kraken.read_kraken_all(cases, classification_folder, readids_max)
        
        features = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, cases, read_tax_depth, tax2depth, info, parents, classification_folder)
        #features = get_features_all(read_names_list, tax2path, kraken_kmers_cases, cases, read_tax_depth, tax2depth, info, parents, classification_folder)
        print(len(features))
        tp_binary_reads_cases = _training.get_tp_binary_reads_cases(cases, read_names_list, tp_cases_dic)
        regr_dic,read_k_prob = _training.train_RF_model_all(cases, features, tp_binary_reads_cases,read_names_list) 
        print(regr_dic)
        best_k_dic,estimated_tax_dict =_training.get_best_tax(read_k_prob,cases,read_names_list,merged,thr_minprob=0.5)

        output_file_name= _utils.write_estimated_tax(estimated_tax_dict,output_file_name=workingdir+"estimated_tax.csv")

    elif mode=="classify":
        pass

        #output_file_name= _utils.write_estimated_tax(estimated_tax_dict,output_file_name=workingdir+"estimated_tax.csv")



if __name__ == "__main__":
    main()
