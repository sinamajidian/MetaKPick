

import argparse
import sys
import os
import logging
import pandas as pd
import numpy as np
import random
import pickle
from datetime import datetime
np.random.seed(42)

import _training
import _classifier
import _utils_tree
import _utils_kraken

# import matplotlib.pyplot as plt
# import seaborn as sns

# import pickle
# from collections import Counter
# import random

np.random.seed(42)



def main():

    logging.basicConfig(level=logging.DEBUG,format='%(asctime)s - %(levelname)s - %(message)s')

    parser = argparse.ArgumentParser(description='Metakpick: for metagenomic classification of reads')
    # # Main operation mode arguments
    #mode_group = parser.add_mutually_exclusive_group(required=True)
    #mode_group.add_argument('--train', action='store_true', help='Run the training of the model')
    #mode_group.add_argument('--classify', action='store_true', help='Run the classification of the model')
    parser.add_argument('--mode', type=str, help='train or classify')
    # parser.add_argument('--workingdir', type=str, help='Working directory')
    # parser.add_argument('--model', type=str, help='Model file')
    # parser.add_argument('--threads', type=int, help='Number of threads')
    parser.add_argument('--kraken_output_folder', type=str, help='Kraken output folder')
    parser.add_argument('--truth_file', type=str, help='Truth file')
    parser.add_argument('--model_folder', type=str, help='Model folder')
    parser.add_argument('--output_file_name', type=str, help='Output file name')
    parser.add_argument('--tree_file', type=str, help='Tree file')
    parser.add_argument('--tax_genome_file', type=str, help='Tax genome file')
    parser.add_argument('--kmer_list', type=str, help='K-mer list file comma separated') # 19,21,23 
    #parser.add_argument('--help', action='store_true', help='show this help message and exit')
    #parser.parse_args(args=None if sys.argv[1:] else ['--help'])
    args = parser.parse_args()
    if len(sys.argv) < 2:
        parser.print_help()
        sys.exit(0)
    
    logging.info("Args: "+str(args))
    mode=args.mode # train or classify
    #workingdir=args.workingdir
    kraken_output_folder=args.kraken_output_folder
    truth_file=args.truth_file
    model_folder=args.model_folder
    output_file_name=args.output_file_name
    tree_file=args.tree_file
    tax_genome_file=args.tax_genome_file
    kmer_list=[] if args.kmer_list is None else [int(kmer) for kmer in args.kmer_list.split(',')]
    logging.info("Input kmer list: "+str(kmer_list))

    logging.info("Starting the program and loading the tax info")
    #workingdir="/vast/blangme2/smajidi5/metagenomics/metakpick_project/"
    #in_folder= workingdir+"files/"
    #tree_file=in_folder+"nodes.dmp"
    #tax_genome_file= in_folder + "seqid2taxid.map_tax_uniq"

    info, Tree, tax_index, tax_genome, parents, tree_df = _utils_tree.get_tax_info(tree_file,tax_genome_file)    
    tax2path, tax2depth = _utils_tree.get_tax2path(tax_genome, info, parents)

    if mode=="train": 
        #folder_input =workingdir+"../changek/simulatation/classification/max15/"
        #truth_file=folder_input+"true_tax_max15.csv" # generated by generate_training.ipynb
        logging.info("Reading the truth file: "+truth_file)
        dic_tax_truth = _utils_kraken.read_truth_file(truth_file)
        logging.info("Number of reads in the truth file: "+str(len(dic_tax_truth)))

        #classification_folder=folder_input 
        cases=[i.split("_")[0] for i in os.listdir(kraken_output_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]
        logging.info("List of kraken indexes  aka cases: "+str(cases))
        if kmer_list:
            cases=[case for case in cases if int(case[1:]) in kmer_list] #'k19'
        logging.info("List of kraken indexes  intersected with the input kmer list: "+str(cases))


        
        #cases=cases[2:]#[4:5]+cases[-1:]
        #logging.info("Reading the kraken classification")
        #merged  = _utils_kraken.read_kraken_classification(cases, truth_file, classification_folder )
        # # todo: get the read set first, then 
        # logging.info("Limiting the reads per genome")
        # num_max=5
        # merged_limited, readids_max = _utils_kraken.limit_read_per_genome(merged,num_max)
            
        # read_names_list=list(readids_max)
        # #cases_dic = _utils_kraken.calculate_tp_fp(dfmerged_taxa,info,tree_df,parents,tax_level,col_name,tax_index)
        # logging.info("Finding the true k size")
        # tp_cases_dic, true_k  =  _utils_kraken.find_true_ksize(cases,merged_limited,info,tree_df,parents,tax_index, readids_max, level='species')
        # logging.info("Reading the kraken kmers")
        # kraken_kmers_cases, read_names_cases = _utils_kraken.read_kraken_all(cases, classification_folder, readids_max)
        # logging.info("Getting the features")
        #logging.info("Mode train started")
        logging.info("Reading the kraken kmers from: "+kraken_output_folder)
        read_names_list, kraken_kmers_cases = _utils_kraken.read_kraken_all(cases, kraken_output_folder)
        # logging.info("Getting the tax depth")
        read_tax_depth = _utils_kraken.get_tax_depth(kraken_kmers_cases, info,parents)
        tax_level='species'
        reads_tp_cases = _utils_kraken.calculate_true_k(kraken_kmers_cases,dic_tax_truth,info,tree_df,parents,tax_level,tax_index,read_names_list)
        
        logging.info("Getting the tp binary reads cases")
        tp_binary_reads_cases = _training.get_tp_binary_reads_cases(cases, read_names_list, reads_tp_cases)
        features_cases, feature_names = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, read_tax_depth, tax2depth, info, parents)
        logging.info("Cases in features: "+str(features_cases.keys()))
        
        logging.info("Training the RF model")
        regr_dic = _training.train_RF_model_all(features_cases, tp_binary_reads_cases,read_names_list,n_estimators=1000, max_features=float(0.8), max_leaf_nodes=50, random_state=14, n_jobs=20) 
        logging.info("Saving the model")
        logging.info(regr_dic)

        now = datetime.now()
        time_stamp_model = now.strftime("%Y%m%d_%H%M%S")
        logging.info("Saving the model in: "+model_folder+"/Random_Forest_regression_models"+time_stamp_model+".pkl")
        pickle.dump(regr_dic, open(model_folder+"/Random_Forest_regression_models"+time_stamp_model+".pkl", "wb"))
        if plot_tree:
            _training.plot_tree(regr_dic, feature_names, model_folder,num_trees=1)
            logging.info("A few example decision trees plotted and saved in: "+model_folder)
    
    elif mode=="classify":
        #folder_input =workingdir+"../changek/simulatation/classification/max15/"
        #classification_folder=folder_input
        #classification_folder="/vast/blangme2/smajidi5/metagenomics/changek/kraken1/classification/cami_soil/long_0/small/"
        #cases=[i.split("_")[0] for i in os.listdir(classification_folder) if i.endswith('_out')] #["k"+str(k) for k in range(15,32)]
        #cases=cases[4:5]+cases[-1:]
        #logging.info("Classifying the reads")
        #logging.info("Loading the model")
        #model_folder=workingdir+"results/"
        model_files =[ model_file for  model_file in os.listdir(model_folder) if model_file.endswith(".pkl") and model_file.startswith("Random_Forest_regression_models")]
        model_files.sort()
        model_file=model_files[-1]
        #model_time_stamp=model_file.split("_")[1] # model_file=workingdir+"Random_Forest_regression_models"+model_time_stamp+".pkl" 
        logging.info("Loading the model: "+model_file+" in the folder: "+model_folder)
        loaded_regression_dic= pickle.load(open(model_folder+model_file, "rb")) # [0] # this is temprorary due to a mistke  remove 



        # remvoe [0] for the new model 




        logging.info("Model loaded"+str(loaded_regression_dic))
        
        #classify_folder=workingdir+"../changek/simulatation/classification/max15/"
        #classify_folder=workingdir+"classification/" # "/vast/blangme2/smajidi5/metagenomics/changek/kraken1/classification/cami_soil/long_0/small/"
        cases_classify =[i.split("_")[0] for i in os.listdir(kraken_output_folder) if i.endswith('_out')]
        cases_model= list(loaded_regression_dic.keys())
        cases_classify_intersect=sorted(list(set(cases_classify).intersection(set(cases_model))))
        logging.info("Cases in the input for classification: "+str(cases_classify))
        logging.info("Cases in the model: "+str(cases_model))
        logging.warning("Working on the intersection of cases in the input for classification and in the model: "+str(cases_classify_intersect))

        
        logging.info("Reading the kraken kmers for these cases")
        read_names_list, kraken_kmers_cases = _utils_kraken.read_kraken_all(cases_classify_intersect, kraken_output_folder)
        logging.info("Number of reads in the kraken kmers: "+str(len(kraken_kmers_cases)))
        logging.info("Getting the tax depth")
        read_tax_depth = _utils_kraken.get_tax_depth(kraken_kmers_cases, info,parents)
        logging.info("Getting the features")
        features_cases, feature_names = _training.get_features_all(read_names_list, tax2path, kraken_kmers_cases, read_tax_depth, tax2depth, info, parents)
        logging.info("Cases in features: "+str(features_cases.keys()))
        logging.info("Applying the model")
        
        read_k_prob= _classifier.apply_RF_model(cases_classify_intersect, features_cases,read_names_list,loaded_regression_dic)
        logging.info("Number of reads in the read_k_prob: "+str(len(read_k_prob)))

        logging.info("Getting the best tax")
        best_k_dic, estimated_tax_dict = _classifier.get_best_tax(read_k_prob,read_names_list,kraken_kmers_cases,thr_minprob=0.5) 
        logging.info("Writing the estimated tax")
        #output_file_name=workingdir+"results/estimated_tax.csv"
        output_file_name= _training.write_estimated_tax(estimated_tax_dict,output_file_name)


        logging.info("Number of reads in the dic_tax_estimated: "+str(len(estimated_tax_dict)))

        #mode_2='stats'
        if truth_file:
            #truth_file="/vast/blangme2/smajidi5/metagenomics/changek/kraken1/classification/cami_soil/long_0_reads_mapping.csv"
            #truth_file=folder_input+"true_tax_max15.csv" # generated by generate_training.ipynb
            logging.info("Reading the truth file: "+truth_file)
            dic_tax_truth = _utils_kraken.read_truth_file(truth_file)
            logging.info("Number of reads in the truth file: "+str(len(dic_tax_truth)))


            logging.info("cleaning reported tax for all kmer sizes")
            kraken_reportedtax_cases=dict()
            kraken_reportedtax_cases['RF']=estimated_tax_dict
            for case in cases_classify_intersect:
                kraken_reportedtax_cases[case]={}
                for read_name, kraken_info in kraken_kmers_cases[case].items():
                    reported_tax = kraken_info[0]
                    kraken_reportedtax_cases[case][read_name]=reported_tax

            logging.info("Calculating the tp fp")
            print("case\tF1\tprecision\trecall\tTP\tFP\tVP")
            for case in list(cases_classify_intersect)+["RF"]:
                logging.info("Calculating the tp fp for case: "+case)
                read_tpfp_dic = _utils_kraken.calculate_tp_fp('predicted_tax',kraken_reportedtax_cases[case],dic_tax_truth,info,tree_df,parents,'species',tax_index)
                logging.info("Number of reads in the TP: "+str(len(read_tpfp_dic['TP'])))

                FP=len(read_tpfp_dic['FP-level-index'])+len(read_tpfp_dic['FP-higher-index'])+len(read_tpfp_dic['FP-level-notindex'])+len(read_tpfp_dic['FP-higher-notindex'])
                recall=len(read_tpfp_dic['TP'])/(len(read_tpfp_dic['TP']) + len(read_tpfp_dic['VP']) + len(read_tpfp_dic['FN']) +FP )
                if len(read_tpfp_dic['TP'])!=0:
                    precision=len(read_tpfp_dic['TP'])/(len(read_tpfp_dic['TP']) + FP)
                    F1= 2* precision* recall/(precision+recall)
                else:
                    precision=0
                    F1=0
                
                print(case,'\t',round(F1,4), round(precision,4),round(recall,4),len(read_tpfp_dic['TP']),FP,len(read_tpfp_dic['VP']))
                        
            

if __name__ == "__main__":
    main()
